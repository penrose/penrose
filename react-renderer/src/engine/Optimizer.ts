import * as tf from "@tensorflow/tfjs";
import {
  Tensor,
  stack,
  scalar,
  Variable,
  Scalar,
  // maximum,
} from "@tensorflow/tfjs";
import {
  argValue,
  evalTranslation,
  insertVaryings,
  genVaryMap,
  evalFns,
} from "./Evaluator";
import Log from "utils/Log";
import { zip } from "lodash";
import { constrDict, objDict } from "contrib/Constraints";

////////////////////////////////////////////////////////////////////////////////
// Globals

// Printing flags
const DEBUG_OPT = false;

// growth factor for constraint weights
const weightGrowthFactor = 10;
// weight for constraints
const constraintWeight = 10e4; // HACK: constant constraint weight
// Intial weight for constraints
const initConstraintWeight = 10e-3;
// learning rate for the tfjs optimizer
// const learningRate = 50; // TODO: Behaves terribly with tree.sub / tree.sty
const learningRate = 30;
const optimizer = tf.train.adam(learningRate, 0.9, 0.999);
// EP method convergence criteria
const epStop = 1e-3;
// const epStop = 1e-5;

// Unconstrained method convergence criteria
// TODO. This should REALLY be 10e-10
// const uoStop = 1e-2;
const uoStop = 1e-5;
// const uoStop = 10;

const EPS = uoStop;

////////////////////////////////////////////////////////////////////////////////

const toPenalty = (x: Tensor): Tensor => {
  return tf.pow(tf.maximum(x, tf.scalar(0)), tf.scalar(2));
};

const epConverged = (
  x0: Tensor,
  x1: Tensor,
  fx0: Scalar,
  fx1: Scalar
): boolean => {
  // TODO: These dx and dfx should really be scaled to account for magnitudes
  const stateChange = sc(x1.sub(x0).norm());
  const energyChange = sc(tf.abs(fx1.sub(fx0)));
  console.debug(
    "epConverged?: stateChange: ",
    stateChange,
    " | energyChange: ",
    energyChange
  );

  return stateChange < epStop || energyChange < epStop;
};

const unconstrainedConverged = (normGrad: Scalar): boolean => {
  console.debug("UO convergence check: ||grad f(x)||", scalarValue(normGrad));
  return scalarValue(normGrad) < uoStop;
};

const applyFn = (f: FnDone<Tensor>, dict: any) => {
  if (dict[f.name]) {
    return dict[f.name](...f.args.map(argValue));
  } else {
    throw new Error(
      `constraint or objective ${f.name} not found in dirctionary`
    );
  }
};

/**
 * Given a `State`, take n steps by evaluating the overall objective function
 *
 * @param {State} state
 * @param {number} steps
 * @returns
 */

// TODO. Annotate the return type: a new (copied?) state with the varyingState and opt params set?

// NOTE: `stepEP` implements the exterior point method as described here:
// https://www.me.utexas.edu/~jensen/ORMM/supplements/units/nlp_methods/const_opt.pdf (p7)

// Things that we should do programmatically improve the conditioning of the objective function:
// 1) scale the constraints so that the penalty generated by each is about the same magnitude
// 2) fix initial value of the penalty parameter so that the magnitude of the penalty term is not much smaller than the magnitude of objective function

export const stepEP = (state: State, steps: number, evaluate = true) => {
  // TODO. Maybe factor this back out into `State -> (VaryingState, OptParams)`?
  const { optStatus, weight } = state.params;
  let newState = { ...state };
  const optParams = newState.params; // this is just a reference, so updating this will update newState as well
  const xs: Variable[] = optParams.mutableUOstate; // also a reference

  console.debug("-------------------");
  console.debug(
    "step EP | weight: ",
    weight,
    "| EP round: ",
    optParams.EPround,
    " | UO round: ",
    optParams.UOround
  );
  console.debug("params: ", optParams);
  // console.debug("state: ", state);
  console.debug("number of varying variables", state.varyingValues.length); // Log these values with scalarValue if needed, but it may block/slow down optimization

  switch (optStatus.tag) {
    case "NewIter": {
      // Collect the overall objective and varying values
      const overallObjective = evalEnergyOn(state); // TODO. Why is this being generated here?

      const newParams: Params = {
        ...state.params,
        // These are made into variables when received by the frontend, in `processData`
        mutableUOstate: state.varyingValues,
        weight: initConstraintWeight,
        UOround: 0,
        EPround: 0,
        optStatus: { tag: "UnconstrainedRunning" },
      };
      // TODO. set `bfgsInfo: defaultBfgsParams`

      return { ...state, params: newParams, overallObjective };
    }

    case "UnconstrainedRunning": {
      if (!state.varyingValues.length) {
        console.error(
          "NOTE: empty state; skipping optimization to evaluate state:",
          evaluate
        ); // Empty state, so don't optimize
        break;
      }

      // NOTE: use cached varying values
      // TODO. we should be using `varyingValues` below in place of `xs`, not the `xs` from optStatus
      // (basically use the last UO state, not last EP state)

      const f = state.overallObjective;
      const fgrad = gradF(f);
      // NOTE: minimize will mutate xs
      const { energy, normGrad } = minimize(f, fgrad, xs, steps);

      // Copy the tensor for xs
      optParams.lastUOstate = tf.clone(tf.stack(xs));
      optParams.lastUOenergy = tf.clone(energy);
      optParams.UOround = optParams.UOround + 1;

      // NOTE: `varyingValues` is updated in `state` after each step by putting it into `newState` and passing it to `evalTranslation`, which returns another state

      // TODO. In the original optimizer, we cheat by using the EP cond here, because the UO cond is sometimes too strong.
      if (unconstrainedConverged(normGrad)) {
        optParams.optStatus.tag = "UnconstrainedConverged"; // TODO. reset bfgs params to default
        console.debug(
          "Unconstrained converged with energy",
          scalarValue(energy)
        );
      } else {
        optParams.optStatus.tag = "UnconstrainedRunning";
        console.debug(
          `Took ${steps} steps. Current energy`,
          scalarValue(energy)
        );
      }

      break;
    }

    case "UnconstrainedConverged": {
      // No minimization step should be taken. Just figure out if we should start another UO round with higher EP weight.
      // We are using the last UO state and energy because they serve as the current EP state and energy, and comparing it to the last EP stuff.

      // Do EP convergence check on the last EP state (and its energy), and curr EP state (and its energy)
      // (There is no EP state or energy on the first round)
      // TODO. Make a diagram to clarify vocabulary
      console.debug("case: unconstrained converged", optParams);

      // We force EP to run at least two rounds (State 0 -> State 1 -> State 2; the first check is only between States 1 and 2)
      if (
        optParams.EPround > 1 &&
        epConverged(
          optParams.lastEPstate,
          optParams.lastUOstate,
          optParams.lastEPenergy,
          optParams.lastUOenergy
        )
      ) {
        optParams.optStatus.tag = "EPConverged";
        console.debug(
          "EP converged with energy",
          scalarValue(optParams.lastUOenergy)
        );
      } else {
        // If EP has not converged, increase weight and continue.
        // The point is that, for the next round, the last converged UO state becomes both the last EP state and the initial state for the next round--starting with a harsher penalty.
        console.debug("EP did not converge; starting next round");
        optParams.optStatus.tag = "UnconstrainedRunning";
        optParams.weight = weightGrowthFactor * weight;
        optParams.EPround = optParams.EPround + 1;
        optParams.UOround = 0;
      }

      // Done with EP check, so save the curr EP state as the last EP state for the future.
      optParams.lastEPstate = tf.clone(optParams.lastUOstate);
      optParams.lastEPenergy = tf.clone(optParams.lastUOenergy);

      break;
    }

    case "EPConverged": // do nothing if converged
      return state;
  }

  // return the state with a new set of shapes
  if (evaluate) {
    // Note: after we finish one evaluation, we "destroy" the AD variables (by making them into scalars) and remake them
    // TODO: Is this the right thing to do?
    const varyingValues = xs
      .map((x) => scalarValue(x as Scalar))
      .map(differentiable);

    newState.translation = insertVaryings(
      state.translation,
      zip(state.varyingPaths, varyingValues) as [Path, Tensor][]
    );

    newState.varyingValues = varyingValues;
    newState = evalTranslation(newState);
  }

  return newState;
};

/**
 * Generate an energy function from the current state
 *
 * @param {State} state
 * @returns a function that takes in a list of `Variable`s and return a `Scalar`
 */
export const evalEnergyOn = (state: State) => {
  const { objFns, constrFns, translation, varyingPaths } = state;
  // TODO: types
  return (...varyingValuesTF: Variable[]): Scalar => {
    // construct a new varying map
    const varyingMap = genVaryMap(varyingPaths, varyingValuesTF) as VaryMap<
      Variable
    >;

    const objEvaled = evalFns(objFns, translation, varyingMap);
    const constrEvaled = evalFns(constrFns, translation, varyingMap);

    const objEngs: Tensor[] = objEvaled.map((o) => applyFn(o, objDict));
    const constrEngs: Tensor[] = constrEvaled.map((c) =>
      toPenalty(applyFn(c, constrDict))
    );

    // console.debug("objEngs", objFns, objEngs, objEngs.map(o => o.dataSync()));

    const objEng: Tensor =
      objEngs.length === 0 ? differentiable(0) : stack(objEngs).sum();
    const constrEng: Tensor =
      constrEngs.length === 0 ? differentiable(0) : stack(constrEngs).sum();
    const overallEng = objEng.add(
      constrEng.mul(scalar(constraintWeight * state.params.weight))
    );

    // NOTE: the current version of tfjs requires all input variables to have gradients (i.e. actually involved when computing the overall energy). See https://github.com/tensorflow/tfjs-core/blob/8c2d9e05643988fa7f4575c30a5ad3e732d189b2/tfjs-core/src/engine.ts#L726
    // HACK: therefore, we try to work around it by using all varying values without affecting the value and gradients of the energy function
    const dummyVal = stack(varyingValuesTF).sum();
    return overallEng.add(dummyVal.mul(scalar(0)));
  };
};

// TODO: This is an old function and should be phased out; may no longer compile
export const step = (state: State, steps: number) => {
  const f = evalEnergyOn(state);
  const fgrad = gradF(f);
  const xs = state.varyingValues;
  // const xs = state.varyingState; // NOTE: use cached varying values
  // NOTE: minimize will mutates xs
  const { energy } = minimize(f, fgrad, xs, steps);
  // insert the resulting variables back into the translation for rendering
  // NOTE: this is a synchronous operation on all varying values; may block
  // Note: after we finish one evaluation, we "destroy" the AD variables (by making them into scalars) and remake them
  // TODO: Is this the right thing to do?
  const varyingValues = xs
    .map((x) => scalarValue(x as Scalar))
    .map(differentiable);
  const trans = insertVaryings(
    state.translation,
    zip(state.varyingPaths, varyingValues) as [Path, Tensor][]
  );
  const newState = { ...state, translation: trans, varyingValues };
  if (scalarValue(energy) > 10) {
    // const newState = { ...state, varyingState: xs };
    newState.params.optStatus.tag = "UnconstrainedRunning";
    console.debug(`Took ${steps} steps. Current energy`, scalarValue(energy));
    // return newState;
  } else {
    // const varyingValues = xs.map((x) => tfStr(x));
    // const trans = insertVaryings(
    //   state.translation,
    //   zip(state.varyingPaths, varyingValues) as [Path, number][]
    // );
    // const newState = { ...state, translation: trans, varyingValues };
    newState.params.optStatus.tag = "EPConverged";
    console.debug("Converged with energy", scalarValue(energy));
    // return evalTranslation(newState);
  }
  // return the state with a new set of shapes
  return evalTranslation(newState);
};

////////////////////////////////////////////////////////////////////////////////
// All TFjs related functions

// TODO: types
export const sc = (x: any): number => x.dataSync()[0];
export const scalarValue = (x: Scalar): number => x.dataSync()[0];
export const tfsStr = (xs: any[]) => xs.map((e) => scalarValue(e));
export const differentiable = (e: number): Variable => tf.scalar(e).variable();
export const gradF = (fn: any) => tf.grads(fn);
export const flatten = (t: Tensor): Tensor => tf.reshape(t, [-1]); // flattens something like Tensor [[1], [2], [3]] (3x1 tensor) into Tensor [1, 2, 3] (1x3)
export const flatten2 = (t: Tensor[]): Tensor => flatten(tf.stack(t));

export const unflatten = (t: Tensor): Tensor[] =>
  tf
    .reshape(t, [t.size, 1])
    .unstack()
    .map((e) => e.asScalar());
// unflatten Tensor [1,2,3] (1x3) into [Tensor 1, Tensor 2, Tensor 3] (3x1) -- since this is the type that f and gradf require as input and output
// The problem is that our data representation assumes a Tensor of size zero (i.e. scalar(3) = Tensor 3), not of size 1 (i.e. Tensor [3])

const awLineSearch = (
  f: (...arg: Tensor[]) => Scalar,
  gradf: (arg: Tensor[]) => Tensor[],
  xs: Tensor,
  gradfx: Tensor, // not nested
  maxSteps = 100
) => {
  // TODO: Do console logs with a debug flag

  const descentDir = tf.neg(gradfx); // TODO: THIS SHOULD BE PRECONDITIONED BY L-BFGS

  const fFlat = (ys: Tensor) => f(...unflatten(ys));
  const gradfxsFlat = (ys: Tensor) => flatten2(gradf(unflatten(ys)));

  const duf = (u: Tensor) => {
    return (ys: Tensor) => {
      const res = u.dot(gradfxsFlat(ys));
      // console.debug("u,xs2", u.arraySync(), xs2.arraySync());
      // console.debug("input", unflatten(xs2));
      // console.debug("e", f(...unflatten(xs2)));
      // console.debug("gu", gradf(unflatten(xs2)));
      // console.debug("gu2", flatten2(gradf(unflatten(xs2))));
      return res;
    };
  };

  const dufDescent = duf(descentDir);
  const dufAtx0 = dufDescent(xs);
  const fAtx0 = fFlat(xs);
  const minInterval = 10e-10;

  // HS: duf, TS: dufDescent
  // HS: x0, TS: xs

  // Hyperparameters
  const c1 = 0.001; // Armijo
  const c2 = 0.9; // Wolfe
  // TODO: Will it cause precision issues to use both tf.scalar and `number`?

  // Armijo condition
  // f(x0 + t * descentDir) <= (f(x0) + c1 * t * <grad(f)(x0), x0>)
  // TODO: Check that these inner lines behave as expected with tf.js
  const armijo = (ti: number) => {
    // TODO: Use addStrict (etc.) everywhere?
    const cond1 = fFlat(xs.addStrict(descentDir.mul(ti)));
    const cond2 = fAtx0.add(dufAtx0.mul(ti * c1));
    // console.debug("armijo", cond1.arraySync(), cond2.arraySync());
    return sc(tf.lessEqualStrict(cond1, cond2));
  };

  // D(u) := <grad f, u>
  // D(u, f, x) = <grad f(x), u>
  // u is the descentDir (i.e. -grad(f)(x))

  // Strong Wolfe condition
  // |<grad(f)(x0 + t * descentDir), u>| <= c2 * |<grad f(x0), u>|
  const strongWolfe = (ti: number) => {
    const cond1 = tf.abs(dufDescent(xs.addStrict(descentDir.mul(ti))));
    const cond2 = tf.abs(dufAtx0).mul(c2);
    return sc(tf.lessEqualStrict(cond1, cond2));
  };

  // Weak Wolfe condition
  // <grad(f)(x0 + t * descentDir), u> >= c2 * <grad f(x0), u>
  const weakWolfe = (ti: number) => {
    const cond1 = dufDescent(xs.addStrict(descentDir.mul(ti)));
    const cond2 = dufAtx0.mul(c2);
    // console.debug("weakWolfe", cond1.arraySync(), cond2.arraySync());
    return sc(tf.greaterEqualStrict(cond1, cond2));
  };

  const wolfe = weakWolfe; // TODO: Set this if using strongWolfe instead

  // Interval check
  const shouldStop = (numUpdates: number, ai: number, bi: number) => {
    const intervalTooSmall = Math.abs(bi - ai) < minInterval;
    const tooManySteps = numUpdates > maxSteps;

    if (intervalTooSmall) {
      console.debug("interval too small");
    }
    if (tooManySteps) {
      console.debug("too many steps");
    }

    return intervalTooSmall || tooManySteps;
  };

  // Consts / initial values
  // TODO: port comments from original

  // const t = 0.002; // for venn_simple.sty
  // const t = 0.1; // for tree.sty

  let a = 0;
  let b = Infinity;
  let t = 1.0;
  let i = 0;

  // console.debug("line search", xs.arraySync(), gradfx.arraySync(), duf(xs)(xs).arraySync());

  // Main loop + update check
  while (true) {
    const needToStop = shouldStop(i, a, b);

    if (needToStop) {
      console.debug("stopping early: (i, a, b, t) = ", i, a, b, t);
      break;
    }

    const isArmijo = armijo(t);
    const isWolfe = wolfe(t);
    // console.debug("(i, a, b, t), armijo, wolfe", i, a, b, t, isArmijo, isWolfe);

    if (!isArmijo) {
      // console.debug("not armijo");
      b = t;
    } else if (!isWolfe) {
      // console.debug("not wolfe");
      a = t;
    } else {
      // console.debug("found good interval");
      // console.debug("stopping: (i, a, b, t) = ", i, a, b, t);
      break;
    }

    if (b < Infinity) {
      // console.debug("already found armijo");
      t = (a + b) / 2.0;
    } else {
      // console.debug("did not find armijo");
      t = 2.0 * a;
    }

    i++;
  }

  return t;
};

/**
 * Use included tf.js optimizer to minimize f over xs (note: xs is mutable)
 *
 * @param {(...arg: tf.Tensor[]) => tf.Tensor} f overall energy function
 * @param {(...arg: tf.Tensor[]) => tf.Tensor[]} gradf gradient function
 * @param {tf.Tensor[]} xs varying state
 * @param {*} names // TODO: what is this
 * @returns // TODO: document
 */
export const minimizePenrose = (
  f: (...arg: Variable[]) => Scalar,
  gradf: (arg: Tensor[]) => Tensor[],
  xs: Variable[],
  maxSteps = 100
): {
  energy: Scalar;
  normGrad: Scalar;
  i: number;
} => {
  // values to be returned
  let energy;
  let i = 0;
  let gradfx;
  let normGrad;

  // TODO: Check that the way this loop is being called (and with # steps) satisfies the requirements of EP (e.g. minimizing an unconstrained problem)

  while (i < maxSteps) {
    // TFJS optimizer
    // energy = optimizer.minimize(() => f(...xs) as any, true);

    // custom optimizer (TODO: factor out)
    // right now, just does vanilla gradient descent with line search
    // Note: tf.clone can clone a variable into a tensor, and after that, the two are unrelated
    // TODO: figure out the best way to dispose/tidy the intermediate tensors

    // TODO: clean this up with the `flatten` function
    // TODO: On iteration, can we save time/space by not reshaping/assigning all these tensors??

    gradfx = tf.stack(gradf(xs));

    const xsCopy = flatten2(xs);
    // const stepSize = awLineSearch(f, gradf, xsCopy, flatten(gradfx));
    const stepSize = 0.002;
    console.debug("stepSize via line search:", stepSize);

    // xs' = xs - dt * grad(f(xs))
    // `stack` makes a new immutable tensor of the vars: Tensor [ v1, v2, v3 ] (where each var is a single-elem list [x])
    // TODO: Can we do this without the arraySync call?
    const xsNew = tf
      .stack(xs)
      .sub(gradfx.mul(stepSize))
      .arraySync();
    // Set each variable to the result
    xs.forEach((e, j) => e.assign(tf.tensor(xsNew[j])));
    energy = f(...xs);
    normGrad = gradfx.norm();

    // note: this printing could tank the performance
    // const vals = xs.map(v => v.dataSync()[0]);
    console.debug("i = ", i);
    // console.debug(`f(xs): ${energy}`);
    // console.debug("f'(xs)", tfsStr(gradfx));
    // console.debug("||f'(xs)||", sc(normGrad));

    i++;
  }

  // const gradfxLast = gradf(xs);
  // Note that tf.stack(gradfx) gives a Tensor of single-element tensors, e.g. Tensor [[-2], [2]]
  // const normGradLast = tf.stack(gradfxLast).norm();

  return { energy: energy as Scalar, normGrad: normGrad as Scalar, i };
};

/**
 * Use included tf.js optimizer to minimize f over xs (note: xs is mutable)
 *
 * @param {(...arg: tf.Tensor[]) => tf.Tensor} f overall energy function
 * @param {(...arg: tf.Tensor[]) => tf.Tensor[]} gradf gradient function
 * @param {tf.Tensor[]} xs varying state
 * @param {*} names // TODO: what is this
 * @returns // TODO: document
 */
export const minimizeTF = (
  f: (...arg: Variable[]) => Scalar,
  gradf: (arg: Tensor[]) => Tensor[],
  xs: Variable[],
  maxSteps = 100
): {
  energy: Scalar;
  normGrad: Scalar;
  i: number;
} => {
  // values to be returned
  let energy;
  let i = 0;
  let normGrad;
  let gradfx;
  let vals;

  while (1) {
    energy = optimizer.minimize(() => f(...xs) as any, true);
    gradfx = gradf(xs);
    normGrad = tf.stack(gradfx).norm();

    if (i >= maxSteps || sc(normGrad) < EPS) {
      if (i >= maxSteps) {
        console.debug("maxSteps reached", maxSteps);
      }
      if (sc(normGrad) < EPS) {
        console.debug("unconstrained converged", sc(normGrad), EPS);
      }
      console.debug("Finished `minimize` early");
      break;
    }

    // note: this printing could tank the performance
    if (DEBUG_OPT) {
      vals = xs.map((v) => v.dataSync()[0]);

      console.debug("i=", i);
      console.debug(`f(xs): ${energy}`);
      console.debug("f'(xs)", tfsStr(gradfx));
      console.debug("||f'(xs)||", sc(normGrad));
      console.debug(
        "stopping conditions",
        i < maxSteps,
        "or",
        sc(normGrad) > EPS
      );
    }

    i++;
  }
  // find the current
  gradfx = gradf(xs);
  normGrad = tf.stack(gradfx).norm();
  return { energy: energy as Scalar, normGrad: normGrad as Scalar, i };
};

export const minimize = minimizeTF;
